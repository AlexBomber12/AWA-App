name: ci

on:
  pull_request:
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  push:
    branches: [main]
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write
  issues: write

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  DOCKER_BUILDKIT: '1'
  COMPOSE_DOCKER_CLI_BUILD: '1'

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Upgrade pip and install base requirements
        run: |
          set -o pipefail
          : > unit-setup.log
          {
            python -m pip install -U pip
            if [ -f requirements-dev.txt ]; then
              python -m pip install -r requirements-dev.txt -c constraints.txt
            fi
            for file in services/*/requirements.txt; do
              [ -f "$file" ] && python -m pip install -r "$file" -c constraints.txt
            done
            python -m pip check || true
          } 2>&1 | tee -a unit-setup.log
          status=${PIPESTATUS[0]}
          python -m pip freeze > pip-freeze.txt || true
          tail -n 200 unit-setup.log > unit-setup-tail.log || true
          if [ "$status" -ne 0 ]; then
            exit "$status"
          fi

      - name: Set up Node.js
        if: ${{ hashFiles('webapp/package.json') != '' }}
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: |
            webapp/package-lock.json

      - name: Install webapp dependencies
        if: ${{ hashFiles('webapp/package.json') != '' }}
        working-directory: webapp
        run: npm ci

      - name: Parse check
        run: python -m compileall -q services tests

      - name: Pytest collect
        run: pytest --collect-only -q

      - name: Python unit tests
        run: |
          if [ -f pyproject.toml ] || [ -f pytest.ini ]; then
            set -o pipefail
            pytest -q --maxfail=1 --disable-warnings | tee unit.log
            test ${PIPESTATUS[0]} -eq 0
          else
            echo "No pytest configuration detected; skipping."
          fi

      - name: Webapp lint
        if: ${{ hashFiles('webapp/package.json') != '' }}
        working-directory: webapp
        run: |
          set -o pipefail
          npm run lint | tee ../eslint.log || true

      - name: Webapp type check
        if: ${{ hashFiles('webapp/package.json') != '' }}
        working-directory: webapp
        run: |
          set -o pipefail
          npx tsc -p . | tee ../tsc.log || true

      - name: Webapp unit tests
        if: ${{ hashFiles('webapp/package.json') != '' }}
        working-directory: webapp
        run: |
          set -o pipefail
          npm run test:unit --if-present | tee ../vitest.log || true

      - name: Docker image build
        run: |
          set -o pipefail
          COMPOSE_FILES=()
          for file in docker-compose.yml docker-compose.ci.yml docker-compose.postgres.yml; do
            [ -f "$file" ] && COMPOSE_FILES+=("-f" "$file")
          done
          docker compose "${COMPOSE_FILES[@]}" build | tee docker-build.log || true
          status=${PIPESTATUS[0]}
          if [ "$status" -ne 0 ]; then
            echo "Docker build exited with status $status" >&2
            exit "$status"
          fi

      - name: Upload pip diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-pip-diagnostics-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            unit-setup.log
            unit-setup-tail.log
            pip-freeze.txt

      - name: Make CI scripts executable
        if: always()
        run: chmod +x scripts/ci/*.sh

      - name: Create debug bundle
        if: always()
        run: |
          ./scripts/ci/make_debug_bundle.sh "debug-bundle-unit-${{ github.run_id }}-${{ github.run_attempt }}.tar.gz"

      - name: Upload unit bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-unit-${{ github.run_id }}-${{ github.run_attempt }}
          path: debug-bundle-unit-${{ github.run_id }}-${{ github.run_attempt }}.tar.gz

  integration:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: unit
    continue-on-error: true
    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Upgrade pip and install requirements
        run: |
          python -m pip install -U pip
          if [ -f requirements-dev.txt ]; then
            python -m pip install -r requirements-dev.txt -c constraints.txt
          fi
          for file in services/*/requirements.txt; do
            [ -f "$file" ] && python -m pip install -r "$file" -c constraints.txt
          done

      - name: Install awa_common (editable)
        run: python -m pip install -e ./packages/awa_common -c constraints.txt

      - name: Verify awa_common import
        run: |
          python -c "import awa_common, sys; import awa_common.dsn; print('awa_common OK', sys.version)"

      - name: Prepare .env
        run: |
          [ -f .env.test ] && cp -f .env.test .env || true

      - name: Compose up api stack
        run: |
          set -euo pipefail
          docker compose up -d --build --wait db redis
          docker compose up -d --build --wait api worker
          docker compose ps > integration-compose-up.log || true

      - name: Wait API/Worker ready
        run: |
          set -euo pipefail
          : > integration-ready.log
          for i in $(seq 1 60); do
            if curl -fsS http://localhost:8000/ready >/dev/null 2>&1 && curl -fsS http://localhost:8001/ready >/dev/null 2>&1; then
              echo "Services on ports 8000 and 8001 are ready" | tee -a integration-ready.log
              exit 0
            fi
            sleep 2
          done
          docker compose logs api worker | tail -n 200 | tee -a integration-ready.log
          exit 1

      - name: API import smoke
        run: |
          set -euo pipefail
          docker compose exec -T api python -c "import awa_common; import sys; print('import-ok', sys.version)"

      - name: awa_common metadata
        run: |
          docker compose exec -T api python -c "import awa_common, import importlib.metadata as m; print(m.version('awa_common'))" || true

      - name: Collect integration logs
        if: always()
        run: |
          set -euo pipefail
          docker compose logs db redis api worker > integration-services.log || true

      - name: Upload integration logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-integration-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            .env
            docker-compose.yml
            docker-compose*.yml
            **/*.log

      - name: Integration tests
        run: |
          set -o pipefail
          pytest -vv -m integration | tee integ.log || true

      - name: Compose down
        if: always()
        run: |
          COMPOSE_FILES=()
          for file in docker-compose.yml docker-compose.ci.yml docker-compose.postgres.yml; do
            [ -f "$file" ] && COMPOSE_FILES+=("-f" "$file")
          done
          docker compose "${COMPOSE_FILES[@]}" down -v || true

  migrations:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: unit
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-python@v6
        with:
          python-version: 3.11

      - name: Install Alembic and deps
        run: |
          python -m pip install -U pip
          if [ -f requirements-dev.txt ]; then python -m pip install -r requirements-dev.txt -c constraints.txt; fi

      - name: Prepare .env
        run: |
          set -euo pipefail
          [ -f .env.test ] && cp -f .env.test .env || true
          cat <<'EOF' > .env.postgres
          PG_HOST=127.0.0.1
          PG_PORT=5432
          PG_USER=postgres
          PG_PASSWORD=postgres
          PG_DATABASE=postgres
          EOF

      - name: Compose up db/redis
        run: |
          set -euo pipefail
          export POSTGRES_USER=postgres
          export POSTGRES_PASSWORD=postgres
          export POSTGRES_DB=postgres
          docker compose up -d --build --wait db redis

      - name: Wait for Postgres
        run: |
          set -euo pipefail
          : > migrations.log
          echo "Waiting for Postgres readiness (timeout 60s)" | tee -a migrations.log
          U=$(docker compose exec -T db bash -lc 'printf %s "${POSTGRES_USER:-postgres}"')
          D=$(docker compose exec -T db bash -lc 'printf %s "${POSTGRES_DB:-$POSTGRES_USER}"')
          READY=0
          for i in $(seq 1 60); do
            if docker compose exec -T db pg_isready -U "$U" -d "$D" >/dev/null 2>&1; then
              READY=1
              echo "Postgres ready after $i attempt(s)" | tee -a migrations.log
              break
            fi
            sleep 1
          done
          if [ "$READY" -ne 1 ]; then
            echo "Postgres did not become ready within timeout" | tee -a migrations.log
            docker compose logs db | tee -a migrations.log
            exit 1
          fi

      - name: Alembic smoke (up→down→up)
        run: |
          set -euo pipefail
          U=$(docker compose exec -T db bash -lc 'printf %s "${POSTGRES_USER:-postgres}"')
          P=$(docker compose exec -T db bash -lc 'printf %s "${POSTGRES_PASSWORD:-postgres}"')
          D=$(docker compose exec -T db bash -lc 'printf %s "${POSTGRES_DB:-$POSTGRES_USER}"')
          PORT=$(docker compose port db 5432 | awk -F: '{print $2}')
          export DATABASE_URL="postgresql+psycopg://${U}:${P}@127.0.0.1:${PORT}/${D}"
          alembic upgrade head
          alembic downgrade -1 || true
          alembic upgrade head

      - name: Upload migrations logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-migrations-${{ github.run_id }}-${{ github.run_attempt }}
          path: |
            .env
            docker-compose.yml
            docker-compose*.yml
            **/alembic.ini
            **/migrations/**
            **/*.log

      - name: Stop database
        if: always()
        run: |
          COMPOSE_FILES=()
          for file in docker-compose.yml docker-compose.ci.yml docker-compose.postgres.yml; do
            [ -f "$file" ] && COMPOSE_FILES+=("-f" "$file")
          done
          docker compose "${COMPOSE_FILES[@]}" down -v || true

  preview:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs:
      - integration
      - migrations
    continue-on-error: true
    env:
      CF_TUNNEL_TOKEN: ${{ secrets.CLOUDFLARE_TUNNEL_TOKEN }}
      CF_TUNNEL_HOSTNAME: ${{ secrets.CLOUDFLARE_TUNNEL_HOSTNAME }}
    if: ${{ github.event_name == 'pull_request' && vars.PREVIEW_ENABLED == '1' }}
    steps:
      - uses: actions/checkout@v5

      - name: Launch preview stack
        run: |
          set -o pipefail
          COMPOSE_FILES=()
          for file in docker-compose.yml docker-compose.ci.yml docker-compose.postgres.yml; do
            [ -f "$file" ] && COMPOSE_FILES+=("-f" "$file")
          done
          : > preview-compose-up.log
          docker compose "${COMPOSE_FILES[@]}" up -d --build | tee -a preview-compose-up.log

      - name: Wait for services
        run: |
          set -o pipefail
          : > preview-ready.log
          (
            set -e
            for service in 8000 8001; do
              ok=0
              for attempt in $(seq 1 60); do
                if curl -fsS "http://localhost:${service}/ready" >/dev/null 2>&1; then
                  ok=1
                  echo "Service on port ${service} is ready"
                  break
                fi
                sleep 2
              done
              if [ "$ok" -ne 1 ]; then
                echo "Service on port ${service} did not become ready" >&2
                exit 1
              fi
            done
          ) 2>&1 | tee -a preview-ready.log
          status=${PIPESTATUS[0]}
          if [ "$status" -ne 0 ]; then
            exit "$status"
          fi

      - name: Start Cloudflare tunnel
        id: tunnel
        if: ${{ env.CF_TUNNEL_TOKEN != '' && env.CF_TUNNEL_HOSTNAME != '' }}
        continue-on-error: true
        env:
          CF_TUNNEL_TOKEN: ${{ env.CF_TUNNEL_TOKEN }}
          CF_TUNNEL_HOSTNAME: ${{ env.CF_TUNNEL_HOSTNAME }}
        run: |
          docker run -d --name awa-preview-tunnel --network host cloudflare/cloudflared:2024.9.0 tunnel --no-autoupdate run --token "$CF_TUNNEL_TOKEN"
          echo "url=${CF_TUNNEL_HOSTNAME}" >> "$GITHUB_OUTPUT"

      - name: Publish preview URL
        id: preview_url
        run: |
          if [ -n "${{ steps.tunnel.outputs.url }}" ]; then
            echo "Preview URL: ${{ steps.tunnel.outputs.url }}" >> "$GITHUB_STEP_SUMMARY"
            printf "%s" "${{ steps.tunnel.outputs.url }}" > preview-url.txt
          else
            echo "Preview URL: n/a" >> "$GITHUB_STEP_SUMMARY"
            printf "n/a" > preview-url.txt
          fi

      - name: Ensure preview placeholder
        if: always()
        run: |
          if [ ! -f preview-url.txt ]; then
            printf "n/a" > preview-url.txt
          fi

      - name: Create debug bundle
        if: always()
        run: |
          ./scripts/ci/make_debug_bundle.sh "debug-bundle-preview-${{ github.run_id }}-${{ github.run_attempt }}.tar.gz"

      - name: Upload preview bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-preview-${{ github.run_id }}-${{ github.run_attempt }}
          path: debug-bundle-preview-${{ github.run_id }}-${{ github.run_attempt }}.tar.gz

      - name: Stop preview stack
        if: always()
        run: |
          COMPOSE_FILES=()
          for file in docker-compose.yml docker-compose.ci.yml docker-compose.postgres.yml; do
            [ -f "$file" ] && COMPOSE_FILES+=("-f" "$file")
          done
          docker compose "${COMPOSE_FILES[@]}" down -v || true
          if docker ps -a --format '{{.Names}}' | grep -q '^awa-preview-tunnel$'; then
            docker rm -f awa-preview-tunnel >/dev/null 2>&1 || true
          fi

  mirror_logs:
    runs-on: ubuntu-latest
    needs:
      - unit
      - integration
      - migrations
    if: ${{ always() }}
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Wait for preview job
        if: ${{ github.event_name == 'pull_request' && vars.PREVIEW_ENABLED == '1' }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          python - <<'PY'
          import json
          import os
          import time
          import urllib.error
          import urllib.request

          token = os.environ.get('GITHUB_TOKEN')
          run_id = os.environ.get('GITHUB_RUN_ID')
          repo = os.environ.get('GITHUB_REPOSITORY')
          if not token or not run_id or not repo:
              raise SystemExit(0)

          def fetch_jobs(url):
              request = urllib.request.Request(
                  url,
                  headers={
                      'Authorization': f'Bearer {token}',
                      'Accept': 'application/vnd.github+json',
                  },
              )
              try:
                  with urllib.request.urlopen(request, timeout=30) as response:
                      payload = json.loads(response.read().decode('utf-8'))
                      link_header = response.headers.get('Link') or ''
              except urllib.error.HTTPError:
                  return [], None
              except urllib.error.URLError:
                  return [], None
              next_url = None
              for part in link_header.split(','):
                  part = part.strip()
                  if part.endswith('rel="next"') and part.startswith('<'):
                      next_url = part[1 : part.find('>')]
                      break
              return payload.get('jobs', []), next_url

          base_url = f'https://api.github.com/repos/{repo}/actions/runs/{run_id}/jobs?per_page=100'
          for attempt in range(60):
              url = base_url
              jobs = []
              while url:
                  batch, url = fetch_jobs(url)
                  jobs.extend(batch)
              preview_job = next((job for job in jobs if job.get('name') == 'preview'), None)
              if not preview_job:
                  time.sleep(10)
                  continue
              if preview_job.get('status') == 'completed':
                  break
              time.sleep(10)
          PY

      - name: Determine context
        id: context
        run: |
          SHORT_SHA="${GITHUB_SHA::8}"
          echo "short_sha=$SHORT_SHA" >> "$GITHUB_OUTPUT"
          if [ "$GITHUB_EVENT_NAME" = "pull_request" ]; then
            echo "event_type=pr" >> "$GITHUB_OUTPUT"
            echo "pr_number=${{ github.event.pull_request.number || '' }}" >> "$GITHUB_OUTPUT"
            if [ "${{ github.event.pull_request.head.repo.full_name || '' }}" = "${{ github.repository }}" ]; then
              echo "is_fork=false" >> "$GITHUB_OUTPUT"
            else
              echo "is_fork=true" >> "$GITHUB_OUTPUT"
            fi
            echo "branch_name=${{ github.event.pull_request.head.ref || '' }}" >> "$GITHUB_OUTPUT"
            MIRROR_PATH="ci-logs/mirror-logs/pr-${{ github.event.pull_request.number || '' }}/latest"
            MIRROR_SUBDIR="mirror-logs/pr-${{ github.event.pull_request.number || '' }}/sha-$SHORT_SHA"
            LATEST_SUBDIR="mirror-logs/pr-${{ github.event.pull_request.number || '' }}/latest"
            echo "mirror_path=$MIRROR_PATH" >> "$GITHUB_OUTPUT"
            echo "mirror_subdir=$MIRROR_SUBDIR" >> "$GITHUB_OUTPUT"
            echo "latest_subdir=$LATEST_SUBDIR" >> "$GITHUB_OUTPUT"
          else
            echo "event_type=push" >> "$GITHUB_OUTPUT"
            echo "is_fork=false" >> "$GITHUB_OUTPUT"
            BRANCH="${GITHUB_REF_NAME:-${GITHUB_REF##*/}}"
            echo "branch_name=$BRANCH" >> "$GITHUB_OUTPUT"
            SAFE_BRANCH="${BRANCH//\//-}"
            echo "branch_slug=$SAFE_BRANCH" >> "$GITHUB_OUTPUT"
            MIRROR_PATH="ci-logs/mirror-logs/branch-${SAFE_BRANCH}/latest"
            MIRROR_SUBDIR="mirror-logs/branch-${SAFE_BRANCH}/sha-$SHORT_SHA"
            LATEST_SUBDIR="mirror-logs/branch-${SAFE_BRANCH}/latest"
            echo "mirror_path=$MIRROR_PATH" >> "$GITHUB_OUTPUT"
            echo "mirror_subdir=$MIRROR_SUBDIR" >> "$GITHUB_OUTPUT"
            echo "latest_subdir=$LATEST_SUBDIR" >> "$GITHUB_OUTPUT"
          fi

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "debug-bundle-*"
          path: artifacts
          merge-multiple: true

      - name: Extract artifacts
        id: extract
        run: |
          mkdir -p digest_logs mirror_ready
          if [ ! -d artifacts ]; then
            exit 0
          fi
          while IFS= read -r -d '' archive; do
            base="$(basename "$archive" .tar.gz)"
            stage="${base#debug-bundle-}"
            stage="${stage%%-*}"
            dest="mirror_ready/${stage}"
            rm -rf "$dest"
            mkdir -p "$dest"
            tar -xzf "$archive" -C "$dest"
            bundle_root="$dest/debug-bundle"
            if [ ! -d "$bundle_root" ]; then
              continue
            fi
            find "$bundle_root" -type f -print0 | while IFS= read -r -d '' file; do
              rel="${file#$bundle_root/}"
              case "$rel" in
                *.log|*.txt|*.out|*.err|*.json|*.xml|*.junit|*.tap)
                  :
                  ;;
                *)
                  continue
                  ;;
              esac
              base_name="$(basename "$rel")"
              flat_rel="${rel//\//-}"
              stage_flat="${stage}-${flat_rel}"
              stage_base="${stage}-${base_name}"
              dest_root="digest_logs/$base_name"
              stage_dest="digest_logs/$stage_flat"
              stage_base_dest="digest_logs/$stage_base"
              tree_dest="digest_logs/${stage}/${rel}"
              mkdir -p "$(dirname "$tree_dest")"
              cp "$file" "$tree_dest"
              if [ ! -f "$dest_root" ]; then
                mkdir -p "$(dirname "$dest_root")"
                cp "$file" "$dest_root"
              fi
              mkdir -p "$(dirname "$stage_dest")"
              cp "$file" "$stage_dest"
              if [ ! -f "$stage_base_dest" ]; then
                cp "$file" "$stage_base_dest"
              fi
            done
          done < <(find artifacts -type f -name '*.tar.gz' -print0)

      - name: Sanitize logs
        run: |
          python - <<'PY'
          import pathlib
          import re

          root_paths = [pathlib.Path('digest_logs'), pathlib.Path('mirror_ready')]
          key_pattern = re.compile(r'([A-Za-z0-9_]*?(?:TOKEN|SECRET|PASSWORD|API_KEY|DSN|AUTH|COOKIE)[A-Za-z0-9_]*=)([^\s]+)', re.IGNORECASE)
          url_pattern = re.compile(r'(://[^:@\s/]+:)([^@\s]+)(@)')

          def sanitize(text: str) -> str:
              for _ in range(2):
                  text = key_pattern.sub(lambda m: m.group(1) + '<redacted>', text)
                  text = url_pattern.sub(lambda m: m.group(1) + '****' + m.group(3), text)
              return text

          for base in root_paths:
              if not base.exists():
                  continue
              for path in base.rglob('*'):
                  if not path.is_file():
                      continue
                  if path.suffix.lower() not in {'.log', '.txt', '.out', '.err', '.json', '.xml', '.junit', '.tap'}:
                      continue
                  try:
                      data = path.read_text(encoding='utf-8', errors='replace')
                  except OSError:
                      continue
                  sanitized = sanitize(data)
                  path.write_text(sanitized, encoding='utf-8')
          PY

      - name: Ensure preview placeholder artifact
        run: |
          mkdir -p digest_logs
          if [ ! -f digest_logs/preview-url.txt ]; then
            printf "n/a" > digest_logs/preview-url.txt
          fi

      - name: Make CI scripts executable
        run: chmod +x scripts/ci/*.sh

      - name: Build digest
        id: digest
        env:
          MIRROR_PATH_VALUE: ${{ steps.context.outputs.mirror_path }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PREVIEW="n/a"
          if [ -f digest_logs/preview-url.txt ]; then
            PREVIEW=$(tr -d '\r\n' < digest_logs/preview-url.txt)
          fi
          MIRROR_PATH="$MIRROR_PATH_VALUE" PREVIEW_URL="$PREVIEW" FAILED_TAIL_LINES=200 ./scripts/ci/make_pr_digest.sh digest_logs
          cat digest_logs/ci-digest.md >> "$GITHUB_STEP_SUMMARY"

      - name: Stage digest for mirror
        run: |
          mkdir -p mirror_ready
          if [ -f digest_logs/ci-digest.md ]; then
            cp digest_logs/ci-digest.md mirror_ready/ci-digest.md
          fi

      - name: Prepare Codex mirror
        run: |
          set -euo pipefail
          RUN_ID="${GITHUB_RUN_ID}"
          OUT=".codex/mirror-logs/${RUN_ID}"
          rm -rf "$OUT"
          mkdir -p "$OUT"
          if [ -d digest_logs ]; then
            cp -r digest_logs/. "$OUT/"
          fi
          if [ -f digest_logs/ci-digest.md ]; then
            cp digest_logs/ci-digest.md .codex/mirror-logs/latest.md
          fi

      - name: Push logs to ci-logs
        if: ${{ steps.context.outputs.is_fork != 'true' }}
        run: |
          if [ ! -d mirror_ready ] || [ -z "$(find mirror_ready -type f -print -quit)" ]; then
            echo "No logs to mirror"
            exit 0
          fi
          MIRROR_SUBDIR="${{ steps.context.outputs.mirror_subdir }}"
          LATEST_SUBDIR="${{ steps.context.outputs.latest_subdir }}"
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git fetch origin ci-logs || true
          if git show-ref --verify --quiet refs/remotes/origin/ci-logs; then
            git worktree add --force ci-logs-worktree origin/ci-logs
          else
            git worktree add --force -b ci-logs ci-logs-worktree
          fi
          git config --global --add safe.directory "$(pwd)/ci-logs-worktree"
          PARENT_DIR="$(dirname "$MIRROR_SUBDIR")"
          mkdir -p "ci-logs-worktree/$PARENT_DIR"
          rsync -a --delete mirror_ready/ "ci-logs-worktree/${MIRROR_SUBDIR}/"
          rm -rf "ci-logs-worktree/${LATEST_SUBDIR}"
          mkdir -p "ci-logs-worktree/${LATEST_SUBDIR}"
          rsync -a mirror_ready/ "ci-logs-worktree/${LATEST_SUBDIR}/"
          cd ci-logs-worktree
          git add "${MIRROR_SUBDIR}" "${LATEST_SUBDIR}"
          if git diff --staged --quiet; then
            echo "No mirror changes"
          else
            if [ "${{ steps.context.outputs.event_type }}" = "pr" ]; then
              CONTEXT="PR #${{ steps.context.outputs.pr_number }}"
            else
              CONTEXT="branch ${{ steps.context.outputs.branch_name }}"
            fi
            git commit -m "[skip ci] Mirror logs for ${CONTEXT}"
            git push origin HEAD:ci-logs
          fi

      - name: Update PR digest comment
        if: ${{ steps.context.outputs.event_type == 'pr' && steps.context.outputs.is_fork != 'true' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const { owner, repo } = context.repo;
            const issue_number = Number('${{ steps.context.outputs.pr_number }}');
            const marker = '<!-- AWA-CI-DIGEST -->';
            const body = fs.readFileSync('digest_logs/ci-digest.md', 'utf8');
            const comments = await github.rest.issues.listComments({ owner, repo, issue_number, per_page: 100 });
            const existing = comments.data.find(comment => comment.body && comment.body.includes(marker));
            const limit = 65000;
            const note = '\n\n_Truncated digest: original length exceeded 65000 characters._\n';
            let payload = body;
            if (payload.length > limit) {
              const sliceLength = Math.max(0, limit - note.length);
              payload = payload.slice(0, sliceLength).trimEnd() + note;
            }
            if (existing) {
              await github.rest.issues.updateComment({ owner, repo, comment_id: existing.id, body: payload });
            } else {
              await github.rest.issues.createComment({ owner, repo, issue_number, body: payload });
            }

      - name: Fork notice
        if: ${{ steps.context.outputs.is_fork == 'true' }}
        run: |
          echo "PR originates from a fork; skipping mirror push and digest comment." >> "$GITHUB_STEP_SUMMARY"
