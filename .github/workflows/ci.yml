name: CI

on:
  pull_request:
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  push:
    branches: [main]
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  workflow_dispatch:

permissions:
  contents: read
  actions: read
  pull-requests: write

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

env:
  DOCKER_BUILDKIT: "1"
  COMPOSE_DOCKER_CLI_BUILD: "1"

jobs:
  prepare-matrix:
    name: prepare matrix
    runs-on: ubuntu-latest
    outputs:
      services: ${{ steps.services.outputs.services }}
      python-version: ${{ steps.python.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect Python version
        id: python
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            const readFile = (filePath) => {
              try {
                return fs.readFileSync(path.join(process.cwd(), filePath), 'utf8');
              } catch (error) {
                return null;
              }
            };

            const parseRequiresPython = (content) => {
              if (!content) {
                return null;
              }
              const requiresMatch = content.match(/requires-python\s*=\s*["']([^"']+)["']/i);
              if (requiresMatch && requiresMatch[1]) {
                const versionMatch = requiresMatch[1].match(/(\d+\.\d+)/);
                if (versionMatch) {
                  return versionMatch[1];
                }
              }
              const poetryMatch = content.match(/^\s*python\s*=\s*["']([^"']+)["']/im);
              if (poetryMatch && poetryMatch[1]) {
                const versionMatch = poetryMatch[1].match(/(\d+\.\d+)/);
                if (versionMatch) {
                  return versionMatch[1];
                }
              }
              const targetMatches = [...content.matchAll(/target-version\s*=\s*\[\s*"py(\d{2,3})"\s*\]/gi)];
              for (const match of targetMatches) {
                const digits = match[1];
                if (digits.length === 2) {
                  return `${digits[0]}.${digits[1]}`;
                }
                if (digits.length === 3) {
                  return `${digits[0]}.${digits.slice(1)}`;
                }
              }
              return null;
            };

            const fromScripts = () => {
              const scriptsDir = path.join(process.cwd(), 'scripts', 'ci');
              try {
                for (const entry of fs.readdirSync(scriptsDir)) {
                  const filePath = path.join(scriptsDir, entry);
                  if (!fs.statSync(filePath).isFile()) continue;
                  const content = fs.readFileSync(filePath, 'utf8');
                  const match = content.match(/PYTHON_VERSION\s*[:=]\s*["']?(\d+\.\d+)["']?/);
                  if (match) {
                    return match[1];
                  }
                }
              } catch (error) {
                // ignore
              }
              return null;
            };

            const candidates = [
              parseRequiresPython(readFile('pyproject.toml')),
              fromScripts(),
            ].filter(Boolean);

            const version = candidates.length > 0 ? candidates[0] : '3.11';
            core.info(`Using Python ${version}`);
            core.setOutput('python-version', version);

      - name: Discover Python services
        id: services
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            const servicesDir = path.join(process.cwd(), 'services');
            let detected = [];

            const hasPythonFiles = (dir) => {
              const queue = [dir];
              while (queue.length) {
                const current = queue.pop();
                let entries = [];
                try {
                  entries = fs.readdirSync(current, { withFileTypes: true });
                } catch (error) {
                  continue;
                }
                for (const entry of entries) {
                  if (entry.name.startsWith('.')) {
                    continue;
                  }
                  const fullPath = path.join(current, entry.name);
                  if (entry.isDirectory()) {
                    queue.push(fullPath);
                  } else if (entry.isFile() && entry.name.endsWith('.py')) {
                    return true;
                  }
                }
              }
              return false;
            };

            try {
              const entries = fs.readdirSync(servicesDir, { withFileTypes: true });
              for (const entry of entries) {
                if (!entry.isDirectory()) {
                  continue;
                }
                if (entry.name.startsWith('.')) {
                  continue;
                }
                const servicePath = path.join(servicesDir, entry.name);
                let hasRequirements = false;
                try {
                  hasRequirements = fs
                    .readdirSync(servicePath)
                    .some((name) => /^requirements.*\.txt$/i.test(name));
                } catch (error) {
                  hasRequirements = false;
                }
                if (hasRequirements || hasPythonFiles(servicePath)) {
                  detected.push(entry.name);
                }
              }
            } catch (error) {
              core.warning(`Unable to inspect services directory: ${error.message}`);
            }

            detected = Array.from(new Set(detected)).sort();
            if (detected.length === 0) {
              detected = ['api'];
              core.warning('No Python services found automatically, defaulting matrix to ["api"].');
            }
            const json = JSON.stringify(detected);
            core.info(`Services matrix: ${json}`);
            core.setOutput('services', json);

      - name: Display matrix
        run: |
          echo "Python version: ${{ steps.python.outputs.python-version }}"
          echo "Services: ${{ steps.services.outputs.services }}"

  lint:
    name: lint
    runs-on: ubuntu-latest
    needs: prepare-matrix
    timeout-minutes: 20
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/**/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install lint dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          python -m pip install -c constraints.txt pre-commit ruff mypy
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: Prepare lint logs
        run: mkdir -p artifacts

      - name: pre-commit
        run: |
          set -o pipefail
          pre-commit run --all-files | tee -a artifacts/lint.log

      - name: Ruff
        run: |
          set -o pipefail
          ruff check . | tee -a artifacts/lint.log

      - name: Mypy
        run: |
          set -o pipefail
          mypy --install-types --non-interactive . | tee -a artifacts/lint.log

      - name: Upload lint logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-lint
          path: artifacts/lint.log
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (lint)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-lint.tar.gz \
          || { echo "::warning::fallback debug bundle (lint)"; tar -czf debug-bundle-lint.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-lint
          path: debug-bundle-lint.tar.gz
          if-no-files-found: ignore

  test:
    name: pytest (${{ matrix.service }})
    runs-on: ubuntu-latest
    needs:
      - prepare-matrix
      - lint
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        service: ${{ fromJson(needs.prepare-matrix.outputs.services) }}
    env:
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/${{ matrix.service }}/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install service dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          if ls services/${{ matrix.service }}/requirements*.txt >/dev/null 2>&1; then
            for req in services/${{ matrix.service }}/requirements*.txt; do
              python -m pip install -r "$req" -c constraints.txt
            done
          fi
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: Determine pytest targets
        id: targets
        env:
          SERVICE_NAME: ${{ matrix.service }}
        run: |
          python <<'PY'
          import json
          import glob
          import os

          service = os.environ["SERVICE_NAME"]
          candidates = []

          for path in (
              f"tests/{service}",
              f"services/{service}/tests",
          ):
              if os.path.isdir(path):
                  candidates.append(path)

          patterns = [
              f"tests/test_{service}.py",
              f"tests/test_{service}_*.py",
              f"tests/{service}_test.py",
              f"tests/{service}/*test*.py",
          ]
          for pattern in patterns:
              for match in glob.glob(pattern):
                  if os.path.isfile(match):
                      candidates.append(match)

          seen = set()
          ordered = []
          for candidate in candidates:
              if candidate not in seen:
                  seen.add(candidate)
                  ordered.append(candidate)

          if not ordered:
              ordered = ["tests"]

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as handle:
              handle.write("paths<<__PYTEST_PATHS__\n")
              handle.write("\n".join(ordered))
              handle.write("\n__PYTEST_PATHS__\n")
              handle.write(f"json={json.dumps(ordered)}\n")
          PY

      - name: Ensure coverage data directory
        run: mkdir -p "coverage-artifacts/${{ matrix.service }}"

      - name: Run pytest
        env:
          SERVICE_NAME: ${{ matrix.service }}
          PYTEST_PATHS: ${{ steps.targets.outputs.paths }}
          COVERAGE_FILE: coverage-artifacts/${{ matrix.service }}/.coverage.${{ matrix.service }}
        run: |
          set -euo pipefail
          mapfile -t TARGETS <<<"${PYTEST_PATHS}"
          if [ "${#TARGETS[@]}" -eq 0 ]; then
            echo "No pytest targets resolved; defaulting to tests"
            TARGETS=("tests")
          fi
          COV_OPTS=()
          if [ -d "services/${SERVICE_NAME}" ]; then
            COV_OPTS+=(--cov="services/${SERVICE_NAME}")
          elif [ -d "packages/${SERVICE_NAME}" ]; then
            COV_OPTS+=(--cov="packages/${SERVICE_NAME}")
          fi
          COV_OPTS+=(--cov="packages/awa_common")
          python -m pytest -q "${TARGETS[@]}" \
            "${COV_OPTS[@]}" \
            --cov-config=.github/coverage.ini \
            --cov-report=term-missing:skip-covered \
            --cov-report=xml:"coverage-${SERVICE_NAME}.xml"
          python -m coverage combine || true
          if [ -f ".coverage" ] && [ ! -f "coverage-artifacts/${SERVICE_NAME}/.coverage.${SERVICE_NAME}" ]; then
            mv .coverage "coverage-artifacts/${SERVICE_NAME}/.coverage.${SERVICE_NAME}" || true
          fi
          if [ ! -f "coverage-artifacts/${SERVICE_NAME}/.coverage.${SERVICE_NAME}" ]; then
            touch "coverage-artifacts/${SERVICE_NAME}/.coverage.${SERVICE_NAME}"
          fi
          python -m coverage report -m > "coverage-${SERVICE_NAME}.txt" || true
          if [ ! -s "coverage-${SERVICE_NAME}.txt" ]; then
            echo "coverage data unavailable" > "coverage-${SERVICE_NAME}.txt"
          fi

      - name: Upload coverage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.service }}
          path: |
            coverage-artifacts/${{ matrix.service }}/.coverage*
            coverage-${{ matrix.service }}.xml
            coverage-${{ matrix.service }}.txt
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (test)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-${{ matrix.service }}.tar.gz \
          || { echo "::warning::fallback debug bundle (test:${{ matrix.service }})"; tar -czf debug-bundle-${{ matrix.service }}.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-${{ matrix.service }}
          path: debug-bundle-${{ matrix.service }}.tar.gz
          if-no-files-found: ignore

  migrations:
    name: migrations
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 30
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: awa
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres -d awa"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 12
    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/awa
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/api/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install migration dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          if ls services/api/requirements*.txt >/dev/null 2>&1; then
            for req in services/api/requirements*.txt; do
              python -m pip install -r "$req" -c constraints.txt
            done
          fi
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: Prepare migrations log
        run: mkdir -p artifacts

      - name: Run migrations round-trip
        run: |
          set -euo pipefail
          {
            echo "== alembic upgrade head =="
            alembic -c services/api/alembic.ini upgrade head
            echo "== alembic downgrade base =="
            alembic -c services/api/alembic.ini downgrade base
            echo "== alembic upgrade head =="
            alembic -c services/api/alembic.ini upgrade head
            echo "== alembic current -v =="
            alembic -c services/api/alembic.ini current -v
          } 2>&1 | tee -a artifacts/migrations.log

      - name: Upload migrations logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: logs-migrations
          path: artifacts/migrations.log
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (migrations)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-migrations.tar.gz \
          || { echo "::warning::fallback debug bundle (migrations)"; tar -czf debug-bundle-migrations.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-migrations
          path: debug-bundle-migrations.tar.gz
          if-no-files-found: ignore

  coverage-aggregate:
    name: coverage aggregate
    runs-on: ubuntu-latest
    needs:
      - prepare-matrix
      - test
    timeout-minutes: 20
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download coverage artifacts
        uses: actions/download-artifact@v4
        with:
          path: coverage-artifacts
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install coverage tooling
        run: |
          python -m pip install -U pip wheel
          python -m pip install coverage diff-cover

      - name: Determine base reference
        id: base
        run: |
          if [ -n "${GITHUB_BASE_REF}" ]; then
            echo "ref=${GITHUB_BASE_REF}" >> "$GITHUB_OUTPUT"
          else
            echo "ref=main" >> "$GITHUB_OUTPUT"
          fi

      - name: Fetch base branch
        run: git fetch origin "${{ steps.base.outputs.ref }}"

      - name: Combine coverage
        run: |
          set -euo pipefail
          python -m coverage erase
          mapfile -t COVERAGE_FILES < <(find coverage-artifacts -type f -name ".coverage*")
          if [ "${#COVERAGE_FILES[@]}" -gt 0 ]; then
            python -m coverage combine "${COVERAGE_FILES[@]}"
            python -m coverage report -m --rcfile=.github/coverage.ini | tee coverage-aggregate.txt
            python -m coverage xml --rcfile=.github/coverage.ini -o coverage-aggregate.xml
          else
            echo "No .coverage* files; falling back to XML aggregation"
            set +e
            python3 scripts/ci/aggregate_coverage_xml.py | tee coverage-aggregate.txt
            STATUS=${PIPESTATUS[0]}
            set -e
            if [ "$STATUS" -ne 0 ] && [ "$STATUS" -ne 2 ]; then
              exit "$STATUS"
            fi
            if [ ! -f coverage-aggregate.xml ]; then
              first_xml=$(find coverage-artifacts -type f -name "coverage-*.xml" | head -n1 || true)
              if [ -n "$first_xml" ]; then
                cp "$first_xml" coverage-aggregate.xml
              else
                cat <<'XML' > coverage-aggregate.xml
<?xml version="1.0" ?>
<coverage branch-rate="0" line-rate="0" version="1.0">
  <packages/>
</coverage>
XML
              fi
            fi
          fi

      - name: Overall coverage info
        if: always()
        run: |
          python <<'PY'
import os
import re
from pathlib import Path

summary_file = os.environ.get('GITHUB_STEP_SUMMARY')
summary_path = Path(summary_file) if summary_file else None
coverage_path = Path('coverage-aggregate.txt')
if coverage_path.exists():
    text = coverage_path.read_text(encoding='utf-8', errors='ignore')
    match = None
    for line in reversed(text.splitlines()):
        if 'TOTAL' in line or '%' in line:
            match = re.search(r"(\d+(?:\.\d+)?)%", line)
            if match:
                break
    if match:
        value = float(match.group(1))
        message = f"Overall coverage (info): {value:.2f}%"
        print(message)
        if summary_path is not None:
            summary_path.parent.mkdir(parents=True, exist_ok=True)
            with summary_path.open('a', encoding='utf-8') as handle:
                handle.write(message + '\n')
        if value < 70.0:
            print(f"::warning::{message}")
    else:
        print("Overall coverage data unavailable")
else:
    print("coverage-aggregate.txt not found")
PY

      - name: Diff coverage gate
        env:
          BASE_REF: ${{ steps.base.outputs.ref }}
        run: |
          set -euo pipefail
          DIFF_BRANCH="origin/${BASE_REF}"
          echo "Comparing against ${DIFF_BRANCH}"
          diff-cover coverage-aggregate.xml --compare-branch="${DIFF_BRANCH}" --fail-under=70 --diff-range-notation=... | tee diff-coverage.txt
          echo "${BASE_REF}" > diff-base.txt

      - name: Upload aggregate coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-aggregate
          path: |
            coverage-aggregate.txt
            coverage-aggregate.xml
            diff-coverage.txt
            diff-base.txt
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (coverage-aggregate)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-coverage-aggregate.tar.gz || true

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-coverage-aggregate
          path: debug-bundle-coverage-aggregate.tar.gz
          if-no-files-found: ignore

  mirror-logs:
    name: mirror-logs
    runs-on: ubuntu-24.04
    needs:
      - lint
      - test
      - migrations
      - coverage-aggregate
    if: always()
    permissions:
      contents: read
      actions: read
      pull-requests: write
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Detect PR comment capability
        id: cap
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ] && [ "${{ github.event.pull_request.head.repo.fork }}" = "false" ]; then
            echo "can_comment=true" >> "$GITHUB_OUTPUT"
          else
            echo "can_comment=false" >> "$GITHUB_OUTPUT"
          fi

      - uses: actions/download-artifact@v4
        with:
          path: artifacts
          merge-multiple: true

      - name: Fetch job URLs
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const run_id = context.runId;
            const jobs = await github.paginate(
              github.rest.actions.listJobsForWorkflowRun,
              { owner, repo, run_id, per_page: 100 }
            );
            const out = jobs.map(job => ({
              name: job.name,
              url: job.html_url,
              status: job.conclusion || job.status,
              id: job.id,
            }));
            const fs = require('fs');
            fs.mkdirSync('artifacts', { recursive: true });
            fs.writeFileSync('artifacts/jobs.json', JSON.stringify(out, null, 2));

      - name: Build PR summary
        run: |
          python3 scripts/ci/make_pr_summary.py artifacts > artifacts/summary.md || echo "No summary" > artifacts/summary.md

      - name: Append summary to job summary
        run: cat artifacts/summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Update PR comment
        if: steps.cap.outputs.can_comment == 'true'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const marker = '<!-- MIRROR_LOGS_SUMMARY -->';
            const summary = fs.readFileSync('artifacts/summary.md','utf8');
            const body = `${marker}\n${summary}\n${marker}`;
            const {owner, repo} = context.repo;
            const pr = context.payload.pull_request.number;
            const { data: comments } = await github.rest.issues.listComments({ owner, repo, issue_number: pr, per_page: 100 });
            const existing = comments.find(c => c.body && c.body.includes(marker));
            if (existing) {
              await github.rest.issues.updateComment({ owner, repo, comment_id: existing.id, body });
            } else {
              await github.rest.issues.createComment({ owner, repo, issue_number: pr, body });
            }

      - uses: actions/upload-artifact@v4
        with:
          name: mirror-logs
          path: artifacts
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (mirror-logs)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-mirror-logs.tar.gz || true

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-mirror-logs
          path: debug-bundle-mirror-logs.tar.gz
          if-no-files-found: ignore
