name: CI

on:
  pull_request:
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  push:
    branches: [main]
    paths-ignore:
      - 'ci-logs/**'
      - '.codex/mirror-logs/**'
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

env:
  DOCKER_BUILDKIT: "1"
  COMPOSE_DOCKER_CLI_BUILD: "1"

jobs:
  prepare-matrix:
    name: prepare matrix
    runs-on: ubuntu-latest
    outputs:
      services: ${{ steps.services.outputs.services }}
      python-version: ${{ steps.python.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Detect Python version
        id: python
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            const readFile = (filePath) => {
              try {
                return fs.readFileSync(path.join(process.cwd(), filePath), 'utf8');
              } catch (error) {
                return null;
              }
            };

            const parseRequiresPython = (content) => {
              if (!content) {
                return null;
              }
              const requiresMatch = content.match(/requires-python\s*=\s*["']([^"']+)["']/i);
              if (requiresMatch && requiresMatch[1]) {
                const versionMatch = requiresMatch[1].match(/(\d+\.\d+)/);
                if (versionMatch) {
                  return versionMatch[1];
                }
              }
              const poetryMatch = content.match(/^\s*python\s*=\s*["']([^"']+)["']/im);
              if (poetryMatch && poetryMatch[1]) {
                const versionMatch = poetryMatch[1].match(/(\d+\.\d+)/);
                if (versionMatch) {
                  return versionMatch[1];
                }
              }
              const targetMatches = [...content.matchAll(/target-version\s*=\s*\[\s*"py(\d{2,3})"\s*\]/gi)];
              for (const match of targetMatches) {
                const digits = match[1];
                if (digits.length === 2) {
                  return `${digits[0]}.${digits[1]}`;
                }
                if (digits.length === 3) {
                  return `${digits[0]}.${digits.slice(1)}`;
                }
              }
              return null;
            };

            const fromScripts = () => {
              const scriptsDir = path.join(process.cwd(), 'scripts', 'ci');
              try {
                for (const entry of fs.readdirSync(scriptsDir)) {
                  const filePath = path.join(scriptsDir, entry);
                  if (!fs.statSync(filePath).isFile()) continue;
                  const content = fs.readFileSync(filePath, 'utf8');
                  const match = content.match(/PYTHON_VERSION\s*[:=]\s*["']?(\d+\.\d+)["']?/);
                  if (match) {
                    return match[1];
                  }
                }
              } catch (error) {
                // ignore
              }
              return null;
            };

            const candidates = [
              parseRequiresPython(readFile('pyproject.toml')),
              fromScripts(),
            ].filter(Boolean);

            const version = candidates.length > 0 ? candidates[0] : '3.11';
            core.info(`Using Python ${version}`);
            core.setOutput('python-version', version);

      - name: Discover Python services
        id: services
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            const servicesDir = path.join(process.cwd(), 'services');
            let detected = [];

            const hasPythonFiles = (dir) => {
              const queue = [dir];
              while (queue.length) {
                const current = queue.pop();
                let entries = [];
                try {
                  entries = fs.readdirSync(current, { withFileTypes: true });
                } catch (error) {
                  continue;
                }
                for (const entry of entries) {
                  if (entry.name.startsWith('.')) {
                    continue;
                  }
                  const fullPath = path.join(current, entry.name);
                  if (entry.isDirectory()) {
                    queue.push(fullPath);
                  } else if (entry.isFile() && entry.name.endsWith('.py')) {
                    return true;
                  }
                }
              }
              return false;
            };

            try {
              const entries = fs.readdirSync(servicesDir, { withFileTypes: true });
              for (const entry of entries) {
                if (!entry.isDirectory()) {
                  continue;
                }
                if (entry.name.startsWith('.')) {
                  continue;
                }
                const servicePath = path.join(servicesDir, entry.name);
                let hasRequirements = false;
                try {
                  hasRequirements = fs
                    .readdirSync(servicePath)
                    .some((name) => /^requirements.*\.txt$/i.test(name));
                } catch (error) {
                  hasRequirements = false;
                }
                if (hasRequirements || hasPythonFiles(servicePath)) {
                  detected.push(entry.name);
                }
              }
            } catch (error) {
              core.warning(`Unable to inspect services directory: ${error.message}`);
            }

            detected = Array.from(new Set(detected)).sort();
            if (detected.length === 0) {
              detected = ['api'];
              core.warning('No Python services found automatically, defaulting matrix to ["api"].');
            }
            const json = JSON.stringify(detected);
            core.info(`Services matrix: ${json}`);
            core.setOutput('services', json);

      - name: Display matrix
        run: |
          echo "Python version: ${{ steps.python.outputs.python-version }}"
          echo "Services: ${{ steps.services.outputs.services }}"

  lint:
    name: lint
    runs-on: ubuntu-latest
    needs: prepare-matrix
    timeout-minutes: 20
    env:
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/**/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install lint dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          python -m pip install -c constraints.txt pre-commit ruff mypy
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: pre-commit
        run: pre-commit run --all-files

      - name: Ruff
        run: ruff check .

      - name: Mypy
        run: mypy --install-types --non-interactive .

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (lint)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-lint.tar.gz \
          || { echo "::warning::fallback debug bundle (lint)"; tar -czf debug-bundle-lint.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-lint
          path: debug-bundle-lint.tar.gz
          if-no-files-found: ignore

  test:
    name: pytest (${{ matrix.service }})
    runs-on: ubuntu-latest
    needs:
      - prepare-matrix
      - lint
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        service: ${{ fromJson(needs.prepare-matrix.outputs.services) }}
    env:
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/${{ matrix.service }}/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install service dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          if ls services/${{ matrix.service }}/requirements*.txt >/dev/null 2>&1; then
            for req in services/${{ matrix.service }}/requirements*.txt; do
              python -m pip install -r "$req" -c constraints.txt
            done
          fi
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: Determine pytest targets
        id: targets
        env:
          SERVICE_NAME: ${{ matrix.service }}
        run: |
          python <<'PY'
          import json
          import glob
          import os

          service = os.environ["SERVICE_NAME"]
          candidates = []

          for path in (
              f"tests/{service}",
              f"services/{service}/tests",
          ):
              if os.path.isdir(path):
                  candidates.append(path)

          patterns = [
              f"tests/test_{service}.py",
              f"tests/test_{service}_*.py",
              f"tests/{service}_test.py",
              f"tests/{service}/*test*.py",
          ]
          for pattern in patterns:
              for match in glob.glob(pattern):
                  if os.path.isfile(match):
                      candidates.append(match)

          seen = set()
          ordered = []
          for candidate in candidates:
              if candidate not in seen:
                  seen.add(candidate)
                  ordered.append(candidate)

          if not ordered:
              ordered = ["tests"]

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as handle:
              handle.write("paths<<__PYTEST_PATHS__\n")
              handle.write("\n".join(ordered))
              handle.write("\n__PYTEST_PATHS__\n")
              handle.write(f"json={json.dumps(ordered)}\n")
          PY

      - name: Run pytest
        env:
          SERVICE_NAME: ${{ matrix.service }}
          PYTEST_PATHS: ${{ steps.targets.outputs.paths }}
        run: |
          set -euo pipefail
          mapfile -t TARGETS <<<"${PYTEST_PATHS}"
          if [ "${#TARGETS[@]}" -eq 0 ]; then
            echo "No pytest targets resolved; defaulting to tests"
            TARGETS=("tests")
          fi
          COV_OPTS=()
          if [ -d "services/${SERVICE_NAME}" ]; then
            COV_OPTS+=(--cov="services/${SERVICE_NAME}")
          elif [ -d "packages/${SERVICE_NAME}" ]; then
            COV_OPTS+=(--cov="packages/${SERVICE_NAME}")
          fi
          COV_OPTS+=(--cov="packages/awa_common")
          python -m pytest -q "${TARGETS[@]}" \
            "${COV_OPTS[@]}" \
            --cov-config=.github/coverage.ini \
            --cov-report=xml:"coverage-${SERVICE_NAME}.xml"

      - name: Normalize coverage data filename
        if: always()
        env:
          SERVICE_NAME: ${{ matrix.service }}
        run: |
          if [ -f ".coverage" ]; then
            mv ".coverage" ".coverage.${SERVICE_NAME}"
          fi

      - name: Coverage summary
        if: always()
        env:
          SERVICE_NAME: ${{ matrix.service }}
        run: |
          if [ -f ".coverage.${SERVICE_NAME}" ]; then
            if ! COVERAGE_FILE=".coverage.${SERVICE_NAME}" python -m coverage report -m > "coverage-${SERVICE_NAME}.txt"; then
              echo "coverage report unavailable" > "coverage-${SERVICE_NAME}.txt"
            fi
          else
            echo "coverage data unavailable" > "coverage-${SERVICE_NAME}.txt"
          fi

      - name: Prepare coverage artifacts
        if: always()
        env:
          SERVICE_NAME: ${{ matrix.service }}
        run: |
          mkdir -p "coverage-artifacts/${SERVICE_NAME}"
          if [ -f ".coverage.${SERVICE_NAME}" ]; then
            cp ".coverage.${SERVICE_NAME}" "coverage-artifacts/${SERVICE_NAME}/"
          fi
          if [ -f "coverage-${SERVICE_NAME}.xml" ]; then
            cp "coverage-${SERVICE_NAME}.xml" "coverage-artifacts/${SERVICE_NAME}/"
          fi
          if [ -f "coverage-${SERVICE_NAME}.txt" ]; then
            cp "coverage-${SERVICE_NAME}.txt" "coverage-artifacts/${SERVICE_NAME}/"
          fi

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.service }}
          path: coverage-artifacts/${{ matrix.service }}
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (test)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-${{ matrix.service }}.tar.gz \
          || { echo "::warning::fallback debug bundle (test:${{ matrix.service }})"; tar -czf debug-bundle-${{ matrix.service }}.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-${{ matrix.service }}
          path: debug-bundle-${{ matrix.service }}.tar.gz
          if-no-files-found: ignore

  migrations:
    name: migrations
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 30
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_DB: awa
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres -d awa"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 12
    env:
      DATABASE_URL: postgresql://postgres:postgres@localhost:5432/awa
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            constraints.txt
            services/api/requirements*.txt

      - name: Upgrade pip tooling
        run: python -m pip install -U pip wheel

      - name: Install migration dependencies
        run: |
          python -m pip install -c constraints.txt -r requirements-dev.txt
          if ls services/api/requirements*.txt >/dev/null 2>&1; then
            for req in services/api/requirements*.txt; do
              python -m pip install -r "$req" -c constraints.txt
            done
          fi
          python -m pip install -c constraints.txt -e packages/awa_common

      - name: Upgrade database schema
        run: alembic -c services/api/alembic.ini upgrade head

      - name: Downgrade database schema to base
        run: alembic -c services/api/alembic.ini downgrade base

      - name: Upgrade database schema (final)
        run: alembic -c services/api/alembic.ini upgrade head

      - name: Show alembic state
        run: alembic -c services/api/alembic.ini current -v

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (migrations)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-migrations.tar.gz \
          || { echo "::warning::fallback debug bundle (migrations)"; tar -czf debug-bundle-migrations.tar.gz \
               constraints.txt .github/workflows/ci.yml || true; }

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-migrations
          path: debug-bundle-migrations.tar.gz
          if-no-files-found: ignore

  coverage-aggregate:
    name: coverage aggregate
    runs-on: ubuntu-latest
    needs:
      - prepare-matrix
      - test
    timeout-minutes: 20
    env:
      PYTHON_VERSION: ${{ needs.prepare-matrix.outputs.python-version }}
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Download coverage artifacts
        uses: actions/download-artifact@v4
        with:
          path: coverage-artifacts
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ needs.prepare-matrix.outputs.python-version }}
          cache: 'pip'

      - name: Install coverage tooling
        run: |
          python -m pip install -U pip wheel
          python -m pip install coverage

      - name: Combine coverage
        run: |
          set -euo pipefail
          python -m coverage erase
          mapfile -t COVERAGE_FILES < <(find coverage-artifacts -type f -name ".coverage*")
          if [ "${#COVERAGE_FILES[@]}" -eq 0 ]; then
            echo "No coverage files found in artifacts"
            exit 1
          fi
          python -m coverage combine "${COVERAGE_FILES[@]}"
          set +e
          python -m coverage report -m --fail-under=70 | tee coverage-aggregate.txt
          STATUS=${PIPESTATUS[0]}
          set -e
          python -m coverage xml -o coverage-aggregate.xml
          cp coverage-aggregate.txt coverage.txt
          exit "${STATUS}"

      - name: Upload aggregate coverage
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-aggregate
          path: |
            coverage-aggregate.txt
            coverage-aggregate.xml
            coverage.txt
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (coverage-aggregate)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-coverage-aggregate.tar.gz || true

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-coverage-aggregate
          path: debug-bundle-coverage-aggregate.tar.gz
          if-no-files-found: ignore

  mirror-logs:
    name: mirror-logs
    runs-on: ubuntu-24.04
    needs:
      - lint
      - test
      - migrations
      - coverage-aggregate
    if: always()
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - uses: actions/download-artifact@v4
        with:
          path: artifacts
          merge-multiple: true

      - name: Build PR summary
        run: |
          python3 scripts/ci/make_pr_summary.py artifacts > artifacts/summary.md || echo "No summary" > artifacts/summary.md

      - name: Job Summary
        run: cat artifacts/summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Update PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const marker = '<!-- MIRROR_LOGS_SUMMARY -->';
            const summary = fs.readFileSync('artifacts/summary.md','utf8');
            const body = `${marker}\n${summary}\n${marker}`;
            const {owner, repo} = context.repo;
            const pr = context.payload.pull_request.number;
            const { data: comments } = await github.rest.issues.listComments({ owner, repo, issue_number: pr, per_page: 100 });
            const existing = comments.find(c => c.body && c.body.includes(marker));
            if (existing) {
              await github.rest.issues.updateComment({ owner, repo, comment_id: existing.id, body });
            } else {
              await github.rest.issues.createComment({ owner, repo, issue_number: pr, body });
            }

      - uses: actions/upload-artifact@v4
        with:
          name: mirror-logs
          path: artifacts
          if-no-files-found: ignore

      - name: Ensure debug bundle script executable
        if: always()
        run: chmod +x scripts/ci/make_debug_bundle.sh || true

      - name: Make debug bundle (mirror-logs)
        if: always()
        run: |
          bash scripts/ci/make_debug_bundle.sh debug-bundle-mirror-logs.tar.gz || true

      - name: Upload debug bundle
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-bundle-mirror-logs
          path: debug-bundle-mirror-logs.tar.gz
          if-no-files-found: ignore
